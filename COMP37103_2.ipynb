{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhirenmalik/comp3710-demo-2/blob/main/COMP37103_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_Fe9Kpxi3_N"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from sklearn.datasets import fetch_lfw_people\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36ca1ba7"
      },
      "source": [
        "# Task\n",
        "Implement a classification neural network for the CIFAR10 dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b40938e6"
      },
      "source": [
        "## Load and preprocess cifar10 data\n",
        "\n",
        "### Subtask:\n",
        "Load the CIFAR10 dataset and perform any necessary preprocessing steps, such as normalization and reshaping.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuo_89qViY1q",
        "outputId": "8a923b46-b73f-465c-cb52-71f3f9bcef55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X: (60000, 3072)\n",
            "Shape of y: (60000,)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "# Fetch the CIFAR10\n",
        "X, y = fetch_openml(\"CIFAR_10\", version=1, return_X_y=True)\n",
        "print(\"Shape of X:\", X.shape)\n",
        "print(\"Shape of y:\", y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfeb88c8"
      },
      "source": [
        "**Reasoning**:\n",
        "Load the CIFAR10 dataset, print the shapes, preprocess the data for CNN, and split into training and testing sets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de320ae0",
        "outputId": "0d2a2a0b-33f2-436a-a24f-6bb911364c84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train_cifar shape: torch.Size([48000, 3, 32, 32])\n",
            "y_train_cifar shape: torch.Size([48000])\n",
            "X_test_cifar shape: torch.Size([12000, 3, 32, 32])\n",
            "y_test_cifar shape: torch.Size([12000])\n"
          ]
        }
      ],
      "source": [
        "# Reshape X to (num_samples, channels, height, width) for PyTorch\n",
        "X_reshaped = X.values.reshape(-1, 3, 32, 32).astype(np.float32)\n",
        "\n",
        "# Normalize the data to the range [0, 1]\n",
        "X_normalized = X_reshaped / 255.0\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_tensor = torch.tensor(X_normalized, dtype=torch.float32)\n",
        "y_tensor = torch.tensor(y.astype('category').cat.codes, dtype=torch.long) # Convert string labels to long tensors\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train_cifar, X_test_cifar, y_train_cifar, y_test_cifar = train_test_split(\n",
        "    X_tensor, y_tensor, test_size=0.2, random_state=42, stratify=y_tensor\n",
        ")\n",
        "\n",
        "print(\"X_train_cifar shape:\", X_train_cifar.shape)\n",
        "print(\"y_train_cifar shape:\", y_train_cifar.shape)\n",
        "print(\"X_test_cifar shape:\", X_test_cifar.shape)\n",
        "print(\"y_test_cifar shape:\", y_test_cifar.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b72d4610"
      },
      "source": [
        "## Define the neural network model, loss function, and optimizer\n",
        "\n",
        "### Subtask:\n",
        "Define a ResNet-18 model for CIFAR10 and choose an appropriate loss function and optimizer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67515b1f"
      },
      "source": [
        "**Reasoning**:\n",
        "Define a ResNet-18 model, modify its final layer for CIFAR10's 10 classes, and set up the Cross-Entropy Loss and Adam optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2OUksOVokyx"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "from typing import Type, List, Optional\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "\n",
        "# ---------------------------\n",
        "# Building block: BasicBlock (modified to match the second implementation)\n",
        "# ---------------------------\n",
        "class BasicBlock(nn.Module):\n",
        "    \"\"\"ResNet-18/34 residual block with two 3x3 convs, matching the second implementation structure.\"\"\"\n",
        "    expansion: int = 1  # output channels multiplier (1 for BasicBlock)\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_planes: int,\n",
        "        planes: int,\n",
        "        stride: int = 1,\n",
        "        norm_layer: Optional[Type[nn.Module]] = None,\n",
        "        bn_decay_rate: float = 0.9, # Added from second implementation\n",
        "        bn_eps: float = 1e-5, # Added from second implementation\n",
        "        kernel_size: int = 3, # Added from second implementation\n",
        "        rate: int = 1, # Added from second implementation for dilation (though unused in this ResNet config)\n",
        "    ):\n",
        "        super().__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "\n",
        "        padding = (kernel_size // 2) * rate # Calculate padding based on kernel and dilation\n",
        "\n",
        "\n",
        "        # Shortcut projection: 1x1 conv + BN (always, to match second implementation)\n",
        "        self.downsample = nn.Sequential(\n",
        "            nn.Conv2d(in_planes, planes * self.expansion, kernel_size=1, stride=stride, bias=False),\n",
        "            norm_layer(planes * self.expansion, eps=bn_eps, momentum=1.0 - bn_decay_rate),\n",
        "        )\n",
        "\n",
        "        # First 3x3 conv\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=kernel_size, stride=stride,\n",
        "                               padding=padding, dilation=rate, bias=False) # Added dilation\n",
        "        self.bn1 = norm_layer(planes, eps=bn_eps, momentum=1.0 - bn_decay_rate)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        # Second 3x3 conv\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=kernel_size, stride=1,\n",
        "                               padding=padding, dilation=rate, bias=False) # Added dilation\n",
        "        self.bn2 = norm_layer(planes, eps=bn_eps, momentum=1.0 - bn_decay_rate)\n",
        "\n",
        "        # Removed dropout layers\n",
        "\n",
        "        # Initialize weights (Kaiming normal for convs)\n",
        "        for m in [self.downsample[0], self.conv1, self.conv2]:\n",
        "             nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
        "\n",
        "\n",
        "    def forward(self, x: torch.Tensor, is_training: bool = True, test_local_stats: bool = False) -> torch.Tensor:\n",
        "        # Note: In PyTorch, BN uses module.train()/eval() to pick batch vs running stats.\n",
        "        # We keep is_training/test_local_stats in the signature for parity but do not override BN behavior here.\n",
        "\n",
        "        identity = self.downsample(x) # Use the always-present downsample\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        # Removed dropout\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        out = out + identity # Residual connection\n",
        "        out = self.relu(out) # ReLU after addition\n",
        "        # Removed dropout\n",
        "        return out\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# ResNet backbone (modified to match the second implementation)\n",
        "# ---------------------------\n",
        "class ResNet(nn.Module):\n",
        "    \"\"\"\n",
        "    ResNet variant matching the provided Haiku version structure.\n",
        "    By default this builds a ResNet-style model using BasicBlock logic.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        block: Type[BasicBlock],\n",
        "        layers: List[int], # This structure is from the original ResNet, second model uses fixed layers\n",
        "        num_classes: int = 10, # Default for CIFAR10\n",
        "        in_channels: int = 3,\n",
        "        norm_layer: Optional[Type[nn.Module]] = None,\n",
        "        bn_decay_rate: float = 0.9, # Added from second implementation\n",
        "        bn_eps: float = 1e-5, # Added from second implementation\n",
        "        channels: int = 64, # Added from second implementation for initial channels\n",
        "    ):\n",
        "        super().__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "\n",
        "        self.inplanes = channels # Start with the specified initial channels\n",
        "\n",
        "        # Initial conv: 3x3, stride=1, no bias + BN + ReLU (matching second implementation)\n",
        "        self.conv1 = nn.Conv2d(in_channels, self.inplanes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1   = norm_layer(self.inplanes, eps=bn_eps, momentum=1.0 - bn_decay_rate)\n",
        "        self.relu  = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.Identity()  # no pooling in the stem, matches second implementation logic\n",
        "\n",
        "        # Stages matching the second implementation's channel progression and structure\n",
        "        # Note: The layer structure [2, 2, 2, 2] is used here for ResNet-18 style blocks,\n",
        "        # but the channel sizes and strides follow the second implementation's pattern.\n",
        "        self.layer1 = self._make_layer(block, channels,  2, stride=1, norm_layer=norm_layer, bn_decay_rate=bn_decay_rate, bn_eps=bn_eps) # channels -> channels\n",
        "        self.layer2 = self._make_layer(block, channels * 2, 2, stride=2, norm_layer=norm_layer, bn_decay_rate=bn_decay_rate, bn_eps=bn_eps) # channels -> channels * 2\n",
        "        self.layer3 = self._make_layer(block, channels * 4, 2, stride=2, norm_layer=norm_layer, bn_decay_rate=bn_decay_rate, bn_eps=bn_eps) # channels * 2 -> channels * 4\n",
        "        self.layer4 = self._make_layer(block, channels * 4, 2, stride=2, norm_layer=norm_layer, bn_decay_rate=bn_decay_rate, bn_eps=bn_eps) # channels * 4 -> channels * 4 (stays at 256)\n",
        "\n",
        "\n",
        "        # Head: global average pool + linear classifier (matching second implementation)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        # Output channels for the final linear layer is based on the last stage's channels\n",
        "        self.fc = nn.Linear(channels * 4 * block.expansion, num_classes) # Use channels * 4\n",
        "        # Initialize final linear layer weights and bias to zero\n",
        "        nn.init.zeros_(self.fc.weight)\n",
        "        nn.init.zeros_(self.fc.bias)\n",
        "\n",
        "\n",
        "        # No _init_weights function called here, initialization is done per module/block\n",
        "\n",
        "    def _make_layer(\n",
        "        self,\n",
        "        block: Type[BasicBlock],\n",
        "        planes: int,\n",
        "        blocks: int,\n",
        "        stride: int,\n",
        "        norm_layer: Type[nn.Module],\n",
        "        bn_decay_rate: float,\n",
        "        bn_eps: float,\n",
        "    ) -> nn.Sequential:\n",
        "        \"\"\"\n",
        "        Create one ResNet stage with `blocks` residual blocks, matching the second implementation's logic.\n",
        "        The first block may downsample spatially via `stride=2`.\n",
        "        \"\"\"\n",
        "        # Downsampling is handled *within* the BasicBlock in the second implementation's logic\n",
        "        # So we don't need a separate downsample module defined here.\n",
        "        # The BasicBlock's internal proj_conv handles the channel/stride matching.\n",
        "\n",
        "        layers = []\n",
        "        # The first block in a stage handles the potential stride and channel change\n",
        "        layers.append(block(self.inplanes, planes, stride, norm_layer, bn_decay_rate, bn_eps))\n",
        "        self.inplanes = planes * block.expansion\n",
        "\n",
        "        # Subsequent blocks in the stage have stride 1 and same channels\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes, 1, norm_layer, bn_decay_rate, bn_eps))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, is_training: bool = True, test_local_stats: bool = False) -> torch.Tensor:\n",
        "      # Note: is_training and test_local_stats are passed to blocks but don't override BN behavior here.\n",
        "      # 32x32 -> 32x32 (stride 1, no pooling)\n",
        "      x = self.conv1(x)\n",
        "      x = self.bn1(x)\n",
        "      x = self.relu(x)\n",
        "      x = self.maxpool(x) # Identity\n",
        "\n",
        "      # Residual stages (spatial sizes and channel progression match second implementation)\n",
        "      x = self.layer1(x, is_training=is_training, test_local_stats=test_local_stats)  # 32x32 -> 32x32, channels\n",
        "      x = self.layer2(x, is_training=is_training, test_local_stats=test_local_stats)  # 32x32 -> 16x16, channels * 2\n",
        "      x = self.layer3(x, is_training=is_training, test_local_stats=test_local_stats)  # 16x16 -> 8x8, channels * 4\n",
        "      x = self.layer4(x, is_training=is_training, test_local_stats=test_local_stats)  # 8x8 -> 4x4, channels * 4\n",
        "\n",
        "\n",
        "      x = self.avgpool(x)         # [B, C, 1, 1]\n",
        "      x = torch.flatten(x, 1)     # [B, C]\n",
        "      x = self.fc(x)              # [B, num_classes]\n",
        "      return x\n",
        "\n",
        "\n",
        "def resnet18(num_classes: int = 10, in_channels: int = 3, dropout_prob: float = 0.0) -> ResNet: # Keep signature but dropout_prob is ignored\n",
        "    \"\"\"Factory for ResNet-18 style model matching the second implementation structure.\"\"\"\n",
        "    # The layer structure [2, 2, 2, 2] from original ResNet18 is used for number of blocks per stage,\n",
        "    # but the channel sizes and strides are fixed within the ResNet class to match the second implementation.\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes=num_classes, in_channels=in_channels, channels=64) # channels=64 matching second impl initial channels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7505b09a",
        "outputId": "07c791d5-1241-4276-c02b-7c1800a4aa50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model defined and moved to: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "\n",
        "# Define the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load a pre-trained ResNet-18 model\n",
        "model = resnet18(num_classes=10, dropout_prob=0).to(device)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
        "\n",
        "# Parameters from the user's working model\n",
        "epochs = 35\n",
        "batch_size = 128\n",
        "decay = 5e-4\n",
        "learning_rate = 0.1\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.0)\n",
        "\n",
        "# Define data augmentation transforms\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4917, 0.4824, 0.4469), (0.2024, 0.1995, 0.2011)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4917, 0.4824, 0.4469), (0.2024, 0.1995, 0.2011)),\n",
        "])\n",
        "\n",
        "# Apply transforms to the data before creating TensorDatasets\n",
        "# Note: For simplicity and to match the user's provided context which doesn't explicitly show\n",
        "# transforms being applied *before* creating the TensorDataset, we will keep the current approach\n",
        "# of having transforms within the DataLoader (or applied implicitly by the DataLoader if using\n",
        "# standard torchvision datasets, which we are not here). The current setup with TensorDataset\n",
        "# and no explicit transform application in the DataLoader means transforms are not being used.\n",
        "# If transforms are desired, they should be applied here or by using torchvision.datasets directly.\n",
        "# For now, we will proceed without explicit transforms applied to the tensors.\n",
        "X_train_cifar_transformed = torch.stack([transform_train(img) for img in X_train_cifar])\n",
        "X_test_cifar_transformed = torch.stack([transform_test(img) for img in X_test_cifar])\n",
        "\n",
        "\n",
        "# Create TensorDatasets from the transformed data\n",
        "train_dataset = TensorDataset(X_train_cifar_transformed, y_train_cifar)\n",
        "test_dataset = TensorDataset(X_test_cifar_transformed, y_test_cifar)\n",
        "\n",
        "\n",
        "# Create DataLoaders from the TensorDatasets\n",
        "# Transforms are not applied here with TensorDataset. If transforms are needed,\n",
        "# consider using torchvision.datasets.CIFAR10 directly or applying transforms\n",
        "# before creating the TensorDataset.\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
        "                          num_workers=2, pin_memory=(device.type==\"cuda\"))\n",
        "val_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,\n",
        "                          num_workers=2, pin_memory=(device.type==\"cuda\"))\n",
        "\n",
        "print(\"Model defined and moved to:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3efa47f9"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "### Subtask:\n",
        "Train the defined ResNet-18 model on the CIFAR10 training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "d07e50b2",
        "outputId": "1a586d94-cdd1-4ccb-c25b-f7ca46c87cfc"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Sequential.forward() got an unexpected keyword argument 'is_training'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1581529137.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;31m# with torch.cuda.amp.autocast(enabled=use_amp): # AMP is not used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Forward pass, pass is_training=True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0;31m# Calculate the loss with L2 regularization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mcross_entropy_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mce_loss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1387049325.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, is_training, test_local_stats)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m       \u001b[0;31m# Residual stages (spatial sizes and channel progression match second implementation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_training\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_local_stats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_local_stats\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 32x32 -> 32x32, channels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_training\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_local_stats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_local_stats\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 32x32 -> 16x16, channels * 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_training\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_local_stats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_local_stats\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 16x16 -> 8x8, channels * 4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Sequential.forward() got an unexpected keyword argument 'is_training'"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from torch.optim import SGD # Import SGD optimizer\n",
        "\n",
        "# Parameters from the user's working model (aligned with the second model)\n",
        "epochs = 35 # Changed to 35\n",
        "batch_size = 128\n",
        "decay = 5e-4             # L2 regularization coefficient (will be added to loss)\n",
        "# learning_rate = 0.05 # Base learning rate\n",
        "learning_rate = 0.1 # Max learning rate for OneCycleLR\n",
        "\n",
        "# OneCycle parameters (approximate optax.linear_onecycle)\n",
        "pct_start = 15.0 / epochs\n",
        "# pct_final = 30./epochs # Not directly used in OneCycleLR in the same way\n",
        "div_factor = 20.0 # Changed to 20.0\n",
        "final_div_factor = 200.0 # Changed to 200.0\n",
        "\n",
        "# Calculate total training steps for the scheduler\n",
        "total_images = len(train_loader.dataset)\n",
        "# total_batch_size = batch_size * (torch.cuda.device_count() if device.type == \"cuda\" else 1) # Not needed with single GPU\n",
        "# num_train_steps = (total_images * epochs) // total_batch_size # Not needed, use steps_per_epoch * epochs\n",
        "steps_per_epoch = len(train_loader)\n",
        "total_steps = steps_per_epoch * epochs\n",
        "\n",
        "# Define the loss function (CrossEntropyLoss)\n",
        "ce_loss_fn = nn.CrossEntropyLoss(reduction='mean')\n",
        "\n",
        "\n",
        "# Define the optimizer (SGD with momentum, no weight_decay here)\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.0)\n",
        "optimizer = SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.0, nesterov=False)\n",
        "\n",
        "\n",
        "# Define a learning rate scheduler (OneCycleLR)\n",
        "scheduler = lr_scheduler.OneCycleLR(optimizer,\n",
        "                                    max_lr=learning_rate,\n",
        "                                    total_steps=total_steps, # Use total_steps\n",
        "                                    pct_start=pct_start,\n",
        "                                    anneal_strategy='linear', # Use linear annealing\n",
        "                                    div_factor=div_factor,\n",
        "                                    final_div_factor=final_div_factor)\n",
        "\n",
        "\n",
        "# use_amp = (device.type == \"cuda\") # AMP is not used in the second model's training loop\n",
        "# scaler = torch.cuda.amp.GradScaler(enabled=use_amp) # Scaler is not used\n",
        "\n",
        "start_time = time.time() # Start timing\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    running_loss = 0.0\n",
        "    # Create a test iterator for periodic evaluation similar to the JAX script\n",
        "    test_iter = iter(val_loader) # Use val_loader for test data\n",
        "\n",
        "    for i, (inputs, labels) in enumerate(train_loader): # Use the DataLoader\n",
        "        inputs = inputs.to(device, non_blocking=True)\n",
        "        labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True) # Set gradients to None for performance\n",
        "\n",
        "        # with torch.cuda.amp.autocast(enabled=use_amp): # AMP is not used\n",
        "        outputs = model(inputs, is_training=True)  # Forward pass, pass is_training=True\n",
        "        # Calculate the loss with L2 regularization\n",
        "        cross_entropy_loss = ce_loss_fn(outputs, labels)\n",
        "\n",
        "        # Add L2 regularization manually (since weight_decay in optimizer applies to all params)\n",
        "        # This matches the user's provided loss function structure\n",
        "        # l2_loss = 0.0 # Calculate L2 loss using the separate function\n",
        "        # for param in model.parameters():\n",
        "        #     if param.requires_grad:\n",
        "        #         l2_loss += torch.norm(param, p=2)**2\n",
        "\n",
        "        # loss = cross_entropy_loss + 0.5 * decay * l2_loss # Multiply L2 loss by 0.5 * decay as in the user's example\n",
        "        reg_loss = decay * l2_regularization(model) # Use the l2_regularization function\n",
        "        loss = cross_entropy_loss + reg_loss\n",
        "\n",
        "\n",
        "        # scaler.scale(loss).backward()  # Backpropagation with scaler # Not using scaler\n",
        "        loss.backward()\n",
        "        # scaler.step(optimizer)  # Update weights with scaler # Not using scaler\n",
        "        optimizer.step()\n",
        "        # scaler.update() # Update the scaler # Not using scaler\n",
        "\n",
        "        # Step the scheduler after each batch\n",
        "        scheduler.step()\n",
        "\n",
        "        running_loss += loss.item() # Accumulate raw loss for averaging\n",
        "\n",
        "        # Print batch loss periodically (optional)\n",
        "        log_interval = 100 # Define log_interval\n",
        "        global_step = epoch * len(train_loader) + i # Calculate global step\n",
        "        if global_step % log_interval == 0:\n",
        "            # Compute train accuracy on current batch\n",
        "            train_acc = accuracy_from_logits(outputs, labels) # Use outputs for accuracy calculation\n",
        "\n",
        "            # Compute test accuracy on one batch (approximate periodic eval)\n",
        "            try:\n",
        "                test_images, test_labels = next(test_iter)\n",
        "            except StopIteration:\n",
        "                test_iter = iter(val_loader) # Reset iterator with val_loader\n",
        "                test_images, test_labels = next(test_iter)\n",
        "            test_images = test_images.to(device, non_blocking=True)\n",
        "            test_labels = test_labels.to(device, non_blocking=True)\n",
        "            with torch.no_grad():\n",
        "                test_logits = model(test_images, is_training=False)\n",
        "                test_acc = accuracy_from_logits(test_logits, test_labels)\n",
        "\n",
        "            print(f\"[Step {global_step}, Loss {loss.item():.5f}] Train / Test accuracy: {train_acc:.3f} / {test_acc:.3f}\")\n",
        "\n",
        "\n",
        "    avg_loss = running_loss / len(train_loader) # Calculate average loss per epoch\n",
        "    print(f'Epoch [{epoch+1}/{epochs}], Training Loss: {avg_loss:.4f}')\n",
        "\n",
        "    # Evaluate on the validation set (full evaluation at end of epoch)\n",
        "    val_acc = evaluate(model, val_loader) # Use the evaluate function\n",
        "    print(f'Epoch [{epoch+1}/{epochs}], Validation Accuracy: {val_acc:.4f}%') # Print as percentage with 4 decimal places\n",
        "\n",
        "\n",
        "end_time = time.time() # End timing\n",
        "total_training_time = end_time - start_time\n",
        "print(f\"Total training time: {total_training_time:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea3f62da"
      },
      "source": [
        "## Evaluate the model\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the trained ResNet-18 model on the CIFAR10 test data and report the classification metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2c9e0b21"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "with torch.inference_mode():\n",
        "    for images, labels in val_loader:\n",
        "        images = images.to(device, non_blocking=True)\n",
        "        labels = labels.to(device, non_blocking=True)\n",
        "        preds = model(images)\n",
        "\n",
        "        all_preds.append(preds.argmax(dim=1).cpu())\n",
        "        all_labels.append(labels.cpu())\n",
        "\n",
        "all_preds = torch.cat(all_preds)\n",
        "all_labels = torch.cat(all_labels)\n",
        "\n",
        "print(classification_report(all_labels, all_preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AJVdwmuxBT7"
      },
      "source": [
        "# Shakes code adapted using chatGPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ssTnn-SwD8T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc2d1694-44b1-4a1d-c13e-8aef6a28c787"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import math\n",
        "import random\n",
        "from typing import Tuple, Dict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import SGD\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "\n",
        "# Reproducibility\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoiOFtA6w-Kb"
      },
      "source": [
        "# Cell 2: Model Definition (ResNet + ResNetBlock)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lcAETc2YwRla"
      },
      "outputs": [],
      "source": [
        "def same_padding(kernel_size: int, dilation: int = 1) -> int:\n",
        "    \"\"\"\n",
        "    Compute SAME padding for odd kernel sizes in 2D.\n",
        "    For a 3x3 kernel, padding = dilation.\n",
        "    \"\"\"\n",
        "    assert kernel_size % 2 == 1, \"SAME padding formula here assumes odd kernel size.\"\n",
        "    return dilation * (kernel_size // 2)\n",
        "\n",
        "class ResNetBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Residual Net Block\n",
        "    Matches the Haiku implementation:\n",
        "    - Projection 1x1 conv + BN on the shortcut path (always, even when stride=1 and channels match).\n",
        "    - Two 3x3 convs each with BN; ReLU after the first BN; final ReLU after addition.\n",
        "    - Supports dilation via 'rate'.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        stride: int,\n",
        "        rate: int = 1,\n",
        "        kernel_size: int = 3,\n",
        "        bn_decay_rate: float = 0.9,\n",
        "        bn_eps: float = 1e-5,\n",
        "        name: str = None\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.name = name\n",
        "        padding = same_padding(kernel_size, rate)\n",
        "\n",
        "        # Shortcut projection: 1x1 conv, stride=stride, no bias\n",
        "        self.proj_conv = nn.Conv2d(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=out_channels,\n",
        "            kernel_size=1,\n",
        "            stride=stride,\n",
        "            padding=0,\n",
        "            bias=False\n",
        "        )\n",
        "        # BatchNorm config: momentum in PyTorch = 1 - decay_rate in Haiku\n",
        "        self.proj_bn = nn.BatchNorm2d(\n",
        "            num_features=out_channels,\n",
        "            eps=bn_eps,\n",
        "            momentum=1.0 - bn_decay_rate,\n",
        "            affine=True,\n",
        "            track_running_stats=True\n",
        "        )\n",
        "\n",
        "        # First conv-bn\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=out_channels,\n",
        "            kernel_size=kernel_size,\n",
        "            stride=stride,\n",
        "            padding=padding,\n",
        "            dilation=rate,\n",
        "            bias=False\n",
        "        )\n",
        "        self.bn1 = nn.BatchNorm2d(\n",
        "            num_features=out_channels,\n",
        "            eps=bn_eps,\n",
        "            momentum=1.0 - bn_decay_rate,\n",
        "            affine=True,\n",
        "            track_running_stats=True\n",
        "        )\n",
        "\n",
        "        # Second conv-bn\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            in_channels=out_channels,\n",
        "            out_channels=out_channels,\n",
        "            kernel_size=kernel_size,\n",
        "            stride=1,\n",
        "            padding=padding,\n",
        "            dilation=rate,\n",
        "            bias=False\n",
        "        )\n",
        "        self.bn2 = nn.BatchNorm2d(\n",
        "            num_features=out_channels,\n",
        "            eps=bn_eps,\n",
        "            momentum=1.0 - bn_decay_rate,\n",
        "            affine=True,\n",
        "            track_running_stats=True\n",
        "        )\n",
        "\n",
        "        # Initialization: Kaiming normal for convs to approximate Haiku's VarianceScaling\n",
        "        for m in [self.proj_conv, self.conv1, self.conv2]:\n",
        "            nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
        "\n",
        "        # BN affine params default to gamma=1, beta=0 (already default)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, is_training: bool = True, test_local_stats: bool = False) -> torch.Tensor:\n",
        "        # Note: In PyTorch, BN uses module.train()/eval() to pick batch vs running stats.\n",
        "        # We keep is_training/test_local_stats in the signature for parity but do not override BN behavior here.\n",
        "\n",
        "        shortcut = self.proj_bn(self.proj_conv(x))\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = F.relu(out, inplace=True)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        out = out + shortcut\n",
        "        out = F.relu(out, inplace=True)\n",
        "        return out\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    \"\"\"\n",
        "    ResNet variant matching the provided Haiku version:\n",
        "    - Initial conv (3x3, stride 1, channels=64 by default) + BN + ReLU\n",
        "    - Layers of residual blocks:\n",
        "        1) 64->64 (stride 1), 64->64 (stride 1)\n",
        "        2) 64->128 (stride 2), 128->128 (stride 1)\n",
        "        3) 128->256 (stride 2), 256->256 (stride 1)\n",
        "        4) 256->256 (stride 2), 256->256 (stride 1)   [note: stays at 256, not 512]\n",
        "    - Global average pooling\n",
        "    - Final Linear to output_size, weight initialized to zeros (and bias zeros) to match hk.Linear(w_init=zeros)\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        output_size: int,\n",
        "        channels: int = 64,\n",
        "        bn_decay_rate: float = 0.9,\n",
        "        bn_eps: float = 1e-5\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.output_size = output_size\n",
        "        self.channels = channels\n",
        "\n",
        "        # Initial conv: 3x3, stride=1, no bias + BN + ReLU\n",
        "        self.initial_conv = nn.Conv2d(\n",
        "            in_channels=3,\n",
        "            out_channels=channels,\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            padding=1,\n",
        "            bias=False\n",
        "        )\n",
        "        self.initial_bn = nn.BatchNorm2d(\n",
        "            num_features=channels,\n",
        "            eps=bn_eps,\n",
        "            momentum=1.0 - bn_decay_rate,\n",
        "            affine=True,\n",
        "            track_running_stats=True\n",
        "        )\n",
        "\n",
        "        nn.init.kaiming_normal_(self.initial_conv.weight, mode='fan_in', nonlinearity='relu')\n",
        "\n",
        "        # Residual blocks as per provided Haiku layout\n",
        "        self.res1a = ResNetBlock(channels, channels, stride=1, rate=1, bn_decay_rate=bn_decay_rate, bn_eps=bn_eps, name=\"resblock_1a\")\n",
        "        self.res1b = ResNetBlock(channels, channels, stride=1, rate=1, bn_decay_rate=bn_decay_rate, bn_eps=bn_eps, name=\"resblock_1b\")\n",
        "\n",
        "        self.res2a = ResNetBlock(channels, channels * 2, stride=2, rate=1, bn_decay_rate=bn_decay_rate, bn_eps=bn_eps, name=\"resblock_2a\")\n",
        "        self.res2b = ResNetBlock(channels * 2, channels * 2, stride=1, rate=1, bn_decay_rate=bn_decay_rate, bn_eps=bn_eps, name=\"resblock_2b\")\n",
        "\n",
        "        self.res3a = ResNetBlock(channels * 2, channels * 4, stride=2, rate=1, bn_decay_rate=bn_decay_rate, bn_eps=bn_eps, name=\"resblock_3a\")\n",
        "        self.res3b = ResNetBlock(channels * 4, channels * 4, stride=1, rate=1, bn_decay_rate=bn_decay_rate, bn_eps=bn_eps, name=\"resblock_3b\")\n",
        "\n",
        "        self.res4a = ResNetBlock(channels * 4, channels * 4, stride=2, rate=1, bn_decay_rate=bn_decay_rate, bn_eps=bn_eps, name=\"resblock_4a\")\n",
        "        self.res4b = ResNetBlock(channels * 4, channels * 4, stride=1, rate=1, bn_decay_rate=bn_decay_rate, bn_eps=bn_eps, name=\"resblock_4b\")\n",
        "\n",
        "        # Final linear: weight zero init to match hk.Linear(w_init=zeros)\n",
        "        self.fc = nn.Linear(channels * 4, output_size, bias=True)\n",
        "        nn.init.zeros_(self.fc.weight)\n",
        "        nn.init.zeros_(self.fc.bias)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, is_training: bool = True, test_local_stats: bool = False) -> torch.Tensor:\n",
        "        x = self.initial_conv(x)\n",
        "        x = self.initial_bn(x)\n",
        "        x = F.relu(x, inplace=True)\n",
        "\n",
        "        x = self.res1a(x, is_training=is_training, test_local_stats=test_local_stats)\n",
        "        x = self.res1b(x, is_training=is_training, test_local_stats=test_local_stats)\n",
        "\n",
        "        x = self.res2a(x, is_training=is_training, test_local_stats=test_local_stats)\n",
        "        x = self.res2b(x, is_training=is_training, test_local_stats=test_local_stats)\n",
        "\n",
        "        x = self.res3a(x, is_training=is_training, test_local_stats=test_local_stats)\n",
        "        x = self.res3b(x, is_training=is_training, test_local_stats=test_local_stats)\n",
        "\n",
        "        x = self.res4a(x, is_training=is_training, test_local_stats=test_local_stats)\n",
        "        x = self.res4b(x, is_training=is_training, test_local_stats=test_local_stats)\n",
        "\n",
        "        # Global average pooling\n",
        "        x = F.adaptive_avg_pool2d(x, output_size=1)  # [N, C, 1, 1]\n",
        "        x = torch.flatten(x, 1)  # [N, C]\n",
        "        logits = self.fc(x)      # [N, output_size]\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgzWEo-Nw7Je"
      },
      "source": [
        "# Cell 3: Data - CIFAR-10 Datasets and DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_h5j9CzGwg5B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "969c40a4-dd81-4fdb-ddf4-81046f8abff6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 170M/170M [00:13<00:00, 12.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 50000 training images\n",
            "No. of Classes: 10\n",
            "Steps per epoch: 391, Total steps: 13685\n"
          ]
        }
      ],
      "source": [
        "# Hyperparameters mirroring the JAX script\n",
        "epochs = 35\n",
        "batch_size = 128\n",
        "decay = 5e-4             # L2 regularization coefficient (will be added to loss)\n",
        "learning_rate = 0.1\n",
        "\n",
        "# OneCycle parameters (approximate optax.linear_onecycle)\n",
        "pct_start = 15.0 / epochs\n",
        "div_factor = 20.0\n",
        "final_div_factor = 200.0\n",
        "\n",
        "# Standard CIFAR-10 normalization\n",
        "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\n",
        "CIFAR10_STD = (0.2470, 0.2435, 0.2616)\n",
        "\n",
        "train_transform = T.Compose([\n",
        "    T.RandomCrop(32, padding=4),\n",
        "    T.RandomHorizontalFlip(),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(CIFAR10_MEAN, CIFAR10_STD),\n",
        "])\n",
        "\n",
        "test_transform = T.Compose([\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(CIFAR10_MEAN, CIFAR10_STD),\n",
        "])\n",
        "\n",
        "# Download and create datasets\n",
        "data_root = \"./data\"\n",
        "train_dataset = torchvision.datasets.CIFAR10(root=data_root, train=True, download=True, transform=train_transform)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root=data_root, train=False, download=True, transform=test_transform)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True,\n",
        "    drop_last=False\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True,\n",
        "    drop_last=False\n",
        ")\n",
        "\n",
        "num_classes = 10\n",
        "total_images = len(train_dataset)\n",
        "steps_per_epoch = math.ceil(total_images / batch_size)\n",
        "total_steps = steps_per_epoch * epochs\n",
        "\n",
        "print(f\"Found {total_images} training images\")\n",
        "print(f\"No. of Classes: {num_classes}\")\n",
        "print(f\"Steps per epoch: {steps_per_epoch}, Total steps: {total_steps}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbqiBtl5w5RD"
      },
      "source": [
        "# Cell 4: Training Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SgXRWpSIwmja"
      },
      "outputs": [],
      "source": [
        "def l2_regularization(model: nn.Module) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Compute 0.5 * sum(||p||^2) over all trainable parameters.\n",
        "    This mirrors the JAX loss-based L2 regularization (not decoupled weight_decay).\n",
        "    \"\"\"\n",
        "    reg = torch.tensor(0.0, device=device)\n",
        "    for p in model.parameters():\n",
        "        if p.requires_grad:\n",
        "            reg = reg + torch.sum(p.pow(2))\n",
        "    return 0.5 * reg\n",
        "\n",
        "@torch.no_grad()\n",
        "def accuracy_from_logits(logits: torch.Tensor, targets: torch.Tensor) -> float:\n",
        "    preds = logits.argmax(dim=1)\n",
        "    return (preds == targets).float().mean().item()\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model: nn.Module, loader: DataLoader) -> float:\n",
        "    model.eval()\n",
        "    total_correct = 0\n",
        "    total_count = 0\n",
        "    for images, labels in loader:\n",
        "        images = images.to(device, non_blocking=True)\n",
        "        labels = labels.to(device, non_blocking=True)\n",
        "        logits = model(images, is_training=False)\n",
        "        preds = logits.argmax(dim=1)\n",
        "        total_correct += (preds == labels).sum().item()\n",
        "        total_count += labels.size(0)\n",
        "    return total_correct / total_count\n",
        "\n",
        "def train_one_epoch(\n",
        "    model: nn.Module,\n",
        "    loader: DataLoader,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    scheduler: OneCycleLR,\n",
        "    epoch: int,\n",
        "    log_interval: int = 100\n",
        "):\n",
        "    model.train()\n",
        "    ce_loss_fn = nn.CrossEntropyLoss(reduction='mean')\n",
        "    running_loss = 0.0\n",
        "    global_step_start = epoch * len(loader)\n",
        "\n",
        "    # Create a test iterator for periodic evaluation similar to the JAX script\n",
        "    test_iter = iter(test_loader)\n",
        "\n",
        "    for batch_idx, (images, labels) in enumerate(loader):\n",
        "        images = images.to(device, non_blocking=True)\n",
        "        labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        logits = model(images, is_training=True)\n",
        "        ce_loss = ce_loss_fn(logits, labels)\n",
        "        reg_loss = decay * l2_regularization(model)\n",
        "        loss = ce_loss + reg_loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        global_step = global_step_start + batch_idx\n",
        "        if global_step % log_interval == 0:\n",
        "            # Compute train accuracy on current batch\n",
        "            train_acc = accuracy_from_logits(logits, labels)\n",
        "\n",
        "            # Compute test accuracy on one batch (approximate periodic eval)\n",
        "            try:\n",
        "                test_images, test_labels = next(test_iter)\n",
        "            except StopIteration:\n",
        "                test_iter = iter(test_loader)\n",
        "                test_images, test_labels = next(test_iter)\n",
        "            test_images = test_images.to(device, non_blocking=True)\n",
        "            test_labels = test_labels.to(device, non_blocking=True)\n",
        "            with torch.no_grad():\n",
        "                test_logits = model(test_images, is_training=False)\n",
        "                test_acc = accuracy_from_logits(test_logits, test_labels)\n",
        "\n",
        "            print(f\"[Step {global_step}, Loss {loss.item():.5f}] Train / Test accuracy: {train_acc:.3f} / {test_acc:.3f}\")\n",
        "\n",
        "    avg_loss = running_loss / len(loader)\n",
        "    return avg_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ok0VG4Ycw3A6"
      },
      "source": [
        "# Cell 5: Instantiate model, optimizer, scheduler, and run training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZAffHS7rwqkq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d7d0af1-ee8f-4800-b4ab-db68f4e9db6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (initial_conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (initial_bn): BatchNorm2d(64, eps=1e-05, momentum=0.09999999999999998, affine=True, track_running_stats=True)\n",
            "  (res1a): ResNetBlock(\n",
            "    (proj_conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (proj_bn): BatchNorm2d(64, eps=1e-05, momentum=0.09999999999999998, affine=True, track_running_stats=True)\n",
            "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.09999999999999998, affine=True, track_running_stats=True)\n",
            "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.09999999999999998, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (res1b): ResNetBlock(\n",
            "    (proj_conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (proj_bn): BatchNorm2d(64, eps=1e-05, momentum=0.09999999999999998, affine=True, track_running_stats=True)\n",
            "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.09999999999999998, affine=True, track_running_stats=True)\n",
            "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.09999999999999998, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (res2a): ResNetBlock(\n",
            "    (proj_conv): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "    (proj_bn): BatchNorm2d(128, eps=1e-05, momentum=0.09999999999999998, affine=True, track_running_stats=True)\n",
            "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.09999999999999998, affine=True, track_running_stats=True)\n",
            "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.09999999999999998, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (res2b): ResNetBlock(\n",
            "    (proj_conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (proj_bn): BatchNorm2d(128, eps=1e-05, momentum=0.09999999999999998, affine=True, track_running_stats=True)\n",
            "    (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.09999999999999998, affine=True, track_running_stats=True)\n",
            "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.09999999999999998, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (res3a): ResNetBlock(\n",
            "    (proj_conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "    (proj_bn): BatchNorm2d(256, eps=1e-05, momentum=0.09999999999999998, affine=True, track_running_stats=True)\n",
            "    (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.09999999999999998, affine=True, track_running_stats=True)\n",
            "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.09999999999999998, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (res3b): ResNetBlock(\n",
            "    (proj_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (proj_bn): BatchNorm2d(256, eps=1e-05, momentum=0.09999999999999998, affine=True, track_running_stats=True)\n",
            "    (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.09999999999999998, affine=True, track_running_stats=True)\n",
            "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.09999999999999998, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (res4a): ResNetBlock(\n",
            "    (proj_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "    (proj_bn): BatchNorm2d(256, eps=1e-05, momentum=0.09999999999999998, affine=True, track_running_stats=True)\n",
            "    (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.09999999999999998, affine=True, track_running_stats=True)\n",
            "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.09999999999999998, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (res4b): ResNetBlock(\n",
            "    (proj_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (proj_bn): BatchNorm2d(256, eps=1e-05, momentum=0.09999999999999998, affine=True, track_running_stats=True)\n",
            "    (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.09999999999999998, affine=True, track_running_stats=True)\n",
            "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.09999999999999998, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (fc): Linear(in_features=256, out_features=10, bias=True)\n",
            ")\n",
            "[Step 0, Loss 5.51548] Train / Test accuracy: 0.117 / 0.102\n",
            "[Step 100, Loss 5.02740] Train / Test accuracy: 0.273 / 0.289\n",
            "[Step 200, Loss 4.74731] Train / Test accuracy: 0.367 / 0.383\n",
            "[Step 300, Loss 4.66137] Train / Test accuracy: 0.414 / 0.438\n",
            "Epoch 1/35, Training Loss: 4.83992, Validation Acc: 0.4694\n",
            "[Step 400, Loss 4.55000] Train / Test accuracy: 0.406 / 0.547\n",
            "[Step 500, Loss 4.23089] Train / Test accuracy: 0.586 / 0.539\n",
            "[Step 600, Loss 4.08266] Train / Test accuracy: 0.578 / 0.586\n",
            "[Step 700, Loss 4.10242] Train / Test accuracy: 0.570 / 0.664\n",
            "Epoch 2/35, Training Loss: 4.03966, Validation Acc: 0.6062\n",
            "[Step 800, Loss 3.66873] Train / Test accuracy: 0.680 / 0.703\n",
            "[Step 900, Loss 3.54050] Train / Test accuracy: 0.648 / 0.602\n",
            "[Step 1000, Loss 3.30779] Train / Test accuracy: 0.789 / 0.695\n",
            "[Step 1100, Loss 3.21957] Train / Test accuracy: 0.750 / 0.758\n",
            "Epoch 3/35, Training Loss: 3.50071, Validation Acc: 0.6833\n",
            "[Step 1200, Loss 3.19834] Train / Test accuracy: 0.758 / 0.750\n",
            "[Step 1300, Loss 3.09946] Train / Test accuracy: 0.758 / 0.758\n",
            "[Step 1400, Loss 2.93287] Train / Test accuracy: 0.820 / 0.766\n",
            "[Step 1500, Loss 2.87559] Train / Test accuracy: 0.773 / 0.758\n",
            "Epoch 4/35, Training Loss: 3.05393, Validation Acc: 0.6926\n",
            "[Step 1600, Loss 2.69710] Train / Test accuracy: 0.820 / 0.797\n",
            "[Step 1700, Loss 2.77514] Train / Test accuracy: 0.734 / 0.789\n",
            "[Step 1800, Loss 2.82400] Train / Test accuracy: 0.742 / 0.797\n",
            "[Step 1900, Loss 2.42817] Train / Test accuracy: 0.844 / 0.805\n",
            "Epoch 5/35, Training Loss: 2.65263, Validation Acc: 0.7468\n",
            "[Step 2000, Loss 2.27486] Train / Test accuracy: 0.875 / 0.812\n",
            "[Step 2100, Loss 2.33777] Train / Test accuracy: 0.820 / 0.797\n",
            "[Step 2200, Loss 2.34855] Train / Test accuracy: 0.758 / 0.828\n",
            "[Step 2300, Loss 2.10704] Train / Test accuracy: 0.844 / 0.820\n",
            "Epoch 6/35, Training Loss: 2.31564, Validation Acc: 0.7920\n",
            "[Step 2400, Loss 2.14700] Train / Test accuracy: 0.812 / 0.836\n",
            "[Step 2500, Loss 2.03266] Train / Test accuracy: 0.820 / 0.828\n",
            "[Step 2600, Loss 2.02722] Train / Test accuracy: 0.836 / 0.867\n",
            "[Step 2700, Loss 1.98888] Train / Test accuracy: 0.805 / 0.828\n",
            "Epoch 7/35, Training Loss: 2.01142, Validation Acc: 0.7674\n",
            "[Step 2800, Loss 1.86647] Train / Test accuracy: 0.859 / 0.867\n",
            "[Step 2900, Loss 1.83764] Train / Test accuracy: 0.836 / 0.820\n",
            "[Step 3000, Loss 1.85923] Train / Test accuracy: 0.812 / 0.844\n",
            "[Step 3100, Loss 1.63078] Train / Test accuracy: 0.859 / 0.805\n",
            "Epoch 8/35, Training Loss: 1.75609, Validation Acc: 0.7892\n",
            "[Step 3200, Loss 1.53978] Train / Test accuracy: 0.898 / 0.859\n",
            "[Step 3300, Loss 1.45217] Train / Test accuracy: 0.906 / 0.828\n",
            "[Step 3400, Loss 1.42987] Train / Test accuracy: 0.852 / 0.805\n",
            "[Step 3500, Loss 1.41939] Train / Test accuracy: 0.867 / 0.820\n",
            "Epoch 9/35, Training Loss: 1.53919, Validation Acc: 0.7973\n",
            "[Step 3600, Loss 1.30361] Train / Test accuracy: 0.883 / 0.812\n",
            "[Step 3700, Loss 1.41562] Train / Test accuracy: 0.836 / 0.797\n",
            "[Step 3800, Loss 1.43845] Train / Test accuracy: 0.828 / 0.883\n",
            "[Step 3900, Loss 1.29220] Train / Test accuracy: 0.836 / 0.859\n",
            "Epoch 10/35, Training Loss: 1.36448, Validation Acc: 0.8149\n",
            "[Step 4000, Loss 1.17716] Train / Test accuracy: 0.867 / 0.898\n",
            "[Step 4100, Loss 1.07891] Train / Test accuracy: 0.875 / 0.820\n",
            "[Step 4200, Loss 1.12767] Train / Test accuracy: 0.867 / 0.867\n",
            "[Step 4300, Loss 1.16160] Train / Test accuracy: 0.850 / 0.859\n",
            "Epoch 11/35, Training Loss: 1.21856, Validation Acc: 0.8372\n",
            "[Step 4400, Loss 1.04998] Train / Test accuracy: 0.906 / 0.875\n",
            "[Step 4500, Loss 1.08692] Train / Test accuracy: 0.852 / 0.789\n",
            "[Step 4600, Loss 1.11508] Train / Test accuracy: 0.852 / 0.867\n",
            "Epoch 12/35, Training Loss: 1.10371, Validation Acc: 0.8218\n",
            "[Step 4700, Loss 1.04171] Train / Test accuracy: 0.859 / 0.820\n",
            "[Step 4800, Loss 1.03314] Train / Test accuracy: 0.867 / 0.820\n",
            "[Step 4900, Loss 1.07203] Train / Test accuracy: 0.859 / 0.820\n",
            "[Step 5000, Loss 1.02125] Train / Test accuracy: 0.852 / 0.859\n",
            "Epoch 13/35, Training Loss: 1.00621, Validation Acc: 0.8388\n",
            "[Step 5100, Loss 0.96513] Train / Test accuracy: 0.898 / 0.891\n",
            "[Step 5200, Loss 0.79758] Train / Test accuracy: 0.922 / 0.852\n",
            "[Step 5300, Loss 1.05826] Train / Test accuracy: 0.812 / 0.898\n",
            "[Step 5400, Loss 1.09808] Train / Test accuracy: 0.812 / 0.828\n",
            "Epoch 14/35, Training Loss: 0.94560, Validation Acc: 0.7456\n",
            "[Step 5500, Loss 0.93255] Train / Test accuracy: 0.898 / 0.852\n",
            "[Step 5600, Loss 0.92813] Train / Test accuracy: 0.844 / 0.820\n",
            "[Step 5700, Loss 0.81196] Train / Test accuracy: 0.883 / 0.883\n",
            "[Step 5800, Loss 0.98780] Train / Test accuracy: 0.836 / 0.844\n",
            "Epoch 15/35, Training Loss: 0.88780, Validation Acc: 0.8124\n",
            "[Step 5900, Loss 0.79834] Train / Test accuracy: 0.891 / 0.852\n",
            "[Step 6000, Loss 0.90113] Train / Test accuracy: 0.859 / 0.781\n",
            "[Step 6100, Loss 0.74435] Train / Test accuracy: 0.891 / 0.852\n",
            "[Step 6200, Loss 0.75641] Train / Test accuracy: 0.922 / 0.852\n",
            "Epoch 16/35, Training Loss: 0.85282, Validation Acc: 0.8100\n",
            "[Step 6300, Loss 0.83805] Train / Test accuracy: 0.891 / 0.875\n",
            "[Step 6400, Loss 0.75278] Train / Test accuracy: 0.898 / 0.844\n",
            "[Step 6500, Loss 0.74196] Train / Test accuracy: 0.891 / 0.867\n",
            "[Step 6600, Loss 0.70024] Train / Test accuracy: 0.930 / 0.898\n",
            "Epoch 17/35, Training Loss: 0.81138, Validation Acc: 0.8293\n",
            "[Step 6700, Loss 0.95952] Train / Test accuracy: 0.867 / 0.836\n",
            "[Step 6800, Loss 0.76287] Train / Test accuracy: 0.906 / 0.828\n",
            "[Step 6900, Loss 0.67975] Train / Test accuracy: 0.930 / 0.828\n",
            "[Step 7000, Loss 0.73466] Train / Test accuracy: 0.930 / 0.852\n",
            "Epoch 18/35, Training Loss: 0.78989, Validation Acc: 0.8383\n",
            "[Step 7100, Loss 0.84623] Train / Test accuracy: 0.867 / 0.930\n",
            "[Step 7200, Loss 0.80944] Train / Test accuracy: 0.867 / 0.844\n",
            "[Step 7300, Loss 0.72710] Train / Test accuracy: 0.922 / 0.867\n",
            "[Step 7400, Loss 0.77149] Train / Test accuracy: 0.891 / 0.898\n",
            "Epoch 19/35, Training Loss: 0.76371, Validation Acc: 0.8390\n",
            "[Step 7500, Loss 0.78918] Train / Test accuracy: 0.828 / 0.898\n",
            "[Step 7600, Loss 0.78362] Train / Test accuracy: 0.883 / 0.844\n",
            "[Step 7700, Loss 0.88776] Train / Test accuracy: 0.852 / 0.820\n",
            "[Step 7800, Loss 0.82801] Train / Test accuracy: 0.883 / 0.898\n",
            "Epoch 20/35, Training Loss: 0.73788, Validation Acc: 0.8161\n",
            "[Step 7900, Loss 0.67944] Train / Test accuracy: 0.898 / 0.906\n",
            "[Step 8000, Loss 0.75255] Train / Test accuracy: 0.930 / 0.836\n",
            "[Step 8100, Loss 0.66520] Train / Test accuracy: 0.930 / 0.875\n",
            "[Step 8200, Loss 0.69608] Train / Test accuracy: 0.906 / 0.914\n",
            "Epoch 21/35, Training Loss: 0.71883, Validation Acc: 0.8602\n",
            "[Step 8300, Loss 0.73755] Train / Test accuracy: 0.898 / 0.859\n",
            "[Step 8400, Loss 0.61008] Train / Test accuracy: 0.953 / 0.844\n",
            "[Step 8500, Loss 0.71588] Train / Test accuracy: 0.883 / 0.875\n",
            "[Step 8600, Loss 0.77611] Train / Test accuracy: 0.891 / 0.844\n",
            "Epoch 22/35, Training Loss: 0.70265, Validation Acc: 0.8288\n",
            "[Step 8700, Loss 0.66235] Train / Test accuracy: 0.906 / 0.906\n",
            "[Step 8800, Loss 0.66462] Train / Test accuracy: 0.906 / 0.875\n",
            "[Step 8900, Loss 0.72951] Train / Test accuracy: 0.875 / 0.891\n",
            "Epoch 23/35, Training Loss: 0.69074, Validation Acc: 0.8658\n",
            "[Step 9000, Loss 0.63681] Train / Test accuracy: 0.922 / 0.875\n",
            "[Step 9100, Loss 0.58524] Train / Test accuracy: 0.938 / 0.828\n",
            "[Step 9200, Loss 0.74311] Train / Test accuracy: 0.891 / 0.914\n",
            "[Step 9300, Loss 0.65760] Train / Test accuracy: 0.922 / 0.891\n",
            "Epoch 24/35, Training Loss: 0.66371, Validation Acc: 0.8668\n",
            "[Step 9400, Loss 0.71818] Train / Test accuracy: 0.883 / 0.883\n",
            "[Step 9500, Loss 0.59801] Train / Test accuracy: 0.930 / 0.812\n",
            "[Step 9600, Loss 0.67919] Train / Test accuracy: 0.891 / 0.828\n",
            "[Step 9700, Loss 0.64726] Train / Test accuracy: 0.898 / 0.891\n",
            "Epoch 25/35, Training Loss: 0.65758, Validation Acc: 0.8799\n",
            "[Step 9800, Loss 0.64373] Train / Test accuracy: 0.914 / 0.906\n",
            "[Step 9900, Loss 0.65156] Train / Test accuracy: 0.883 / 0.797\n",
            "[Step 10000, Loss 0.71215] Train / Test accuracy: 0.883 / 0.867\n",
            "[Step 10100, Loss 0.71629] Train / Test accuracy: 0.898 / 0.828\n",
            "Epoch 26/35, Training Loss: 0.63681, Validation Acc: 0.8579\n",
            "[Step 10200, Loss 0.62006] Train / Test accuracy: 0.922 / 0.906\n",
            "[Step 10300, Loss 0.73036] Train / Test accuracy: 0.859 / 0.898\n",
            "[Step 10400, Loss 0.67815] Train / Test accuracy: 0.906 / 0.852\n",
            "[Step 10500, Loss 0.55463] Train / Test accuracy: 0.930 / 0.875\n",
            "Epoch 27/35, Training Loss: 0.61488, Validation Acc: 0.8879\n",
            "[Step 10600, Loss 0.57498] Train / Test accuracy: 0.938 / 0.898\n",
            "[Step 10700, Loss 0.51970] Train / Test accuracy: 0.938 / 0.859\n",
            "[Step 10800, Loss 0.60689] Train / Test accuracy: 0.930 / 0.914\n",
            "[Step 10900, Loss 0.63651] Train / Test accuracy: 0.914 / 0.859\n",
            "Epoch 28/35, Training Loss: 0.59928, Validation Acc: 0.8633\n",
            "[Step 11000, Loss 0.56675] Train / Test accuracy: 0.930 / 0.922\n",
            "[Step 11100, Loss 0.52184] Train / Test accuracy: 0.961 / 0.891\n",
            "[Step 11200, Loss 0.52955] Train / Test accuracy: 0.930 / 0.875\n",
            "[Step 11300, Loss 0.47968] Train / Test accuracy: 0.953 / 0.875\n",
            "Epoch 29/35, Training Loss: 0.57578, Validation Acc: 0.9053\n",
            "[Step 11400, Loss 0.48918] Train / Test accuracy: 0.953 / 0.875\n",
            "[Step 11500, Loss 0.51606] Train / Test accuracy: 0.938 / 0.844\n",
            "[Step 11600, Loss 0.50759] Train / Test accuracy: 0.961 / 0.883\n",
            "[Step 11700, Loss 0.54391] Train / Test accuracy: 0.938 / 0.867\n",
            "Epoch 30/35, Training Loss: 0.54936, Validation Acc: 0.8838\n",
            "[Step 11800, Loss 0.43844] Train / Test accuracy: 0.977 / 0.922\n",
            "[Step 11900, Loss 0.47010] Train / Test accuracy: 0.977 / 0.867\n",
            "[Step 12000, Loss 0.56332] Train / Test accuracy: 0.922 / 0.914\n",
            "[Step 12100, Loss 0.50143] Train / Test accuracy: 0.930 / 0.898\n",
            "Epoch 31/35, Training Loss: 0.51601, Validation Acc: 0.9026\n",
            "[Step 12200, Loss 0.49956] Train / Test accuracy: 0.922 / 0.914\n",
            "[Step 12300, Loss 0.50478] Train / Test accuracy: 0.945 / 0.867\n",
            "[Step 12400, Loss 0.43137] Train / Test accuracy: 0.977 / 0.883\n",
            "[Step 12500, Loss 0.42416] Train / Test accuracy: 0.977 / 0.930\n",
            "Epoch 32/35, Training Loss: 0.47647, Validation Acc: 0.9143\n",
            "[Step 12600, Loss 0.39135] Train / Test accuracy: 0.984 / 0.922\n",
            "[Step 12700, Loss 0.43218] Train / Test accuracy: 0.953 / 0.891\n",
            "[Step 12800, Loss 0.38481] Train / Test accuracy: 0.977 / 0.875\n",
            "[Step 12900, Loss 0.37120] Train / Test accuracy: 0.977 / 0.922\n",
            "Epoch 33/35, Training Loss: 0.43691, Validation Acc: 0.9217\n",
            "[Step 13000, Loss 0.36824] Train / Test accuracy: 0.984 / 0.906\n",
            "[Step 13100, Loss 0.32946] Train / Test accuracy: 1.000 / 0.930\n",
            "[Step 13200, Loss 0.40521] Train / Test accuracy: 0.969 / 0.930\n",
            "Epoch 34/35, Training Loss: 0.38650, Validation Acc: 0.9318\n",
            "[Step 13300, Loss 0.37965] Train / Test accuracy: 0.969 / 0.938\n",
            "[Step 13400, Loss 0.37716] Train / Test accuracy: 0.977 / 0.922\n",
            "[Step 13500, Loss 0.32731] Train / Test accuracy: 1.000 / 0.922\n",
            "[Step 13600, Loss 0.32049] Train / Test accuracy: 1.000 / 0.930\n",
            "Epoch 35/35, Training Loss: 0.34924, Validation Acc: 0.9397\n",
            "Training took 411.20 secs or 6.85 mins in total\n"
          ]
        }
      ],
      "source": [
        "model = ResNet(output_size=num_classes, channels=64, bn_decay_rate=0.9, bn_eps=1e-5).to(device)\n",
        "\n",
        "# Optimizer: SGD with momentum; no weight_decay here (we add L2 to the loss explicitly)\n",
        "optimizer = SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.0, nesterov=False)\n",
        "\n",
        "# OneCycleLR: approximate optax.linear_onecycle_schedule behavior\n",
        "scheduler = OneCycleLR(\n",
        "    optimizer,\n",
        "    max_lr=learning_rate,\n",
        "    total_steps=total_steps,\n",
        "    pct_start=pct_start,          # warmup fraction ~ 15/epochs\n",
        "    anneal_strategy='linear',\n",
        "    div_factor=div_factor,        # initial lr = max_lr / div_factor\n",
        "    final_div_factor=final_div_factor\n",
        ")\n",
        "\n",
        "print(model)\n",
        "\n",
        "start_time = time.time()\n",
        "for epoch in range(epochs):\n",
        "    avg_loss = train_one_epoch(model, train_loader, optimizer, scheduler, epoch, log_interval=100)\n",
        "    # End-of-epoch eval (optional)\n",
        "    val_acc = evaluate(model, test_loader)\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {avg_loss:.5f}, Validation Acc: {val_acc:.4f}\")\n",
        "\n",
        "elapsed = time.time() - start_time\n",
        "print(f\"Training took {elapsed:.2f} secs or {elapsed/60:.2f} mins in total\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCsEFM_wwzjI"
      },
      "source": [
        "# Cell 6: Final evaluation on the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZaZmfBAwt3z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e44ce6c-ea86-4e8e-9aec-4aec224c1589"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "top_1_acc: 0.9397\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.98      0.98      1200\n",
            "           1       0.99      1.00      0.99      1200\n",
            "           2       0.98      0.98      0.98      1200\n",
            "           3       0.96      0.96      0.96      1200\n",
            "           4       0.98      0.97      0.98      1200\n",
            "           5       0.97      0.96      0.96      1200\n",
            "           6       0.98      1.00      0.99      1200\n",
            "           7       0.99      0.99      0.99      1200\n",
            "           8       0.99      0.99      0.99      1200\n",
            "           9       0.99      0.99      0.99      1200\n",
            "\n",
            "    accuracy                           0.98     12000\n",
            "   macro avg       0.98      0.98      0.98     12000\n",
            "weighted avg       0.98      0.98      0.98     12000\n",
            "\n",
            "END\n"
          ]
        }
      ],
      "source": [
        "final_test_acc = evaluate(model, test_loader)\n",
        "print(\"top_1_acc:\", final_test_acc)\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "with torch.inference_mode():\n",
        "    for images, labels in val_loader:\n",
        "        images = images.to(device, non_blocking=True)\n",
        "        labels = labels.to(device, non_blocking=True)\n",
        "        preds = model(images)\n",
        "\n",
        "        all_preds.append(preds.argmax(dim=1).cpu())\n",
        "        all_labels.append(labels.cpu())\n",
        "\n",
        "all_preds = torch.cat(all_preds)\n",
        "all_labels = torch.cat(all_labels)\n",
        "\n",
        "print(classification_report(all_labels, all_preds))\n",
        "print(\"END\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}